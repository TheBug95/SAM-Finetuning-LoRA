# SAM LoRA Fine-tuning

## ğŸ“ Estructura del Proyecto

```text
SAM finetuning LoRA/
â”‚
â”œâ”€â”€ Core Modules/              # MÃ³dulos principales del framework
â”‚   â”œâ”€â”€ __init__.py           # InicializaciÃ³n del paquete
â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n del proyecto
â”‚   â”œâ”€â”€ dataset.py            # Carga y procesamiento de datos COCO
â”‚   â”œâ”€â”€ model.py              # Modelo SAM con LoRA
â”‚   â”œâ”€â”€ trainer.py            # LÃ³gica de entrenamiento
â”‚   â””â”€â”€ utils.py              # Utilidades (losses, mÃ©tricas, etc.)
â”‚
â”œâ”€â”€ Main Scripts/              # Scripts principales de ejecuciÃ³n
â”‚   â”œâ”€â”€ train.py              # Script de entrenamiento
â”‚   â”œâ”€â”€ inference.py          # Script de inferencia/testing
â”‚   â””â”€â”€ optuna_tuning.py      # OptimizaciÃ³n de hiperparÃ¡metros
â”‚
â”œâ”€â”€ Utility Scripts/           # Scripts auxiliares
â”‚   â”œâ”€â”€ quickstart.py         # Interfaz interactiva para comenzar
â”‚   â”œâ”€â”€ verify_setup.py       # VerificaciÃ³n de instalaciÃ³n
â”‚   â”œâ”€â”€ export_to_huggingface.py  # Exportar a HuggingFace Hub
â”‚   â”œâ”€â”€ prepare_for_colab.py  # Preparar paquete para Google Colab
â”‚   â””â”€â”€ run_training.ps1      # Script PowerShell con menÃº
â”‚
â”œâ”€â”€ SAM_LoRA_Fine_tuning_colab_setup.ipynb  # Notebook para Google Colab
â”œâ”€â”€ requirements.txt           # Dependencias de Python
â”œâ”€â”€ advanced_examples.py       # Ejemplos de uso avanzado
â””â”€â”€ .gitignore                # Archivos a ignorar en Git
```

## ğŸš€ CÃ³mo Usar 

### Google Colab

Para usar el proyecto en Google Colab:

```bash
# Usar directamente el notebook incluido
# Subir SAM_LoRA_Fine_tuning_colab_setup.ipynb a Google Colab
```


## ğŸ“ Notas Importantes

### 1. Imports AutomÃ¡ticos

Los scripts en `Main Scripts` y `Utility Scripts` automÃ¡ticamente agregan `Core Modules` al path de Python, por lo que no necesitas preocuparte por los imports.

### 2. Rutas Relativas

Todos los scripts usan rutas relativas correctas:

- Los scripts buscan archivos relativos al directorio raÃ­z del proyecto
- Los checkpoints se guardan en `outputs/` desde la raÃ­z
- Los datos se buscan en la ruta especificada en `--data_root`

### 3. VerificaciÃ³n del Setup

Antes de empezar, ejecuta:

```bash
python "Utility Scripts/verify_setup.py"
```

Este script verifica:

- âœ“ VersiÃ³n de Python
- âœ“ PyTorch y CUDA
- âœ“ Dependencias instaladas
- âœ“ Checkpoint de SAM
- âœ“ Datos COCO
- âœ“ Estructura de archivos
- âœ“ Imports de mÃ³dulos

### Uso en Google Colab

** Usar notebook directamente**

1. Subir `SAM_LoRA_Fine_tuning_colab_setup.ipynb` a Google Colab
2. Seguir las instrucciones en el notebook
3. El notebook incluye todo lo necesario para entrenar

## âœ… Checklist de InstalaciÃ³n

Antes de comenzar a entrenar, verifica que todo estÃ© configurado:

- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] Checkpoint de SAM descargado en `checkpoints/sam_vit_b_01ec64.pth`
- [ ] Dataset COCO preparado y accesible
- [ ] `verify_setup.py` ejecutado exitosamente sin errores
- [ ] CUDA disponible (opcional, pero recomendado para entrenamiento)

## ğŸ“ Scripts Disponibles

### Scripts Principales (`Main Scripts/`)

- **`train.py`**: Script principal de entrenamiento con LoRA
- **`inference.py`**: EvaluaciÃ³n y testing del modelo entrenado
- **`optuna_tuning.py`**: OptimizaciÃ³n automÃ¡tica de hiperparÃ¡metros

### Scripts Utilitarios (`Utility Scripts/`)

- **`quickstart.py`**: Interfaz interactiva para comenzar rÃ¡pidamente
- **`verify_setup.py`**: VerificaciÃ³n completa del entorno
- **`export_to_huggingface.py`**: Exportar modelo a HuggingFace Hub
- **`prepare_for_colab.py`**: Preparar paquete optimizado para Google Colab
- **`run_training.ps1`**: Script PowerShell con menÃº interactivo (Windows)

Este proyecto utiliza SAM (Segment Anything Model) de Meta AI.

Las contribuciones son bienvenidas. Por favor, abre un issue o pull request para mejoras o correcciones.

---
